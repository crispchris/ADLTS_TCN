{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define TCN components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1ABpioijZrb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Adding problem\n",
    "\n",
    "## The Adding Problem\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this task, each input consists of a length-T sequence of depth 2, with all values randomly\n",
    "chosen randomly in [0, 1] in dimension 1. The second dimension consists of all zeros except for\n",
    "two elements, which are marked by 1. The objective is to sum the two random values whose second \n",
    "dimensions are marked by 1. One can think of this as computing the dot product of two dimensions.\n",
    "\n",
    "Simply predicting the sum to be 1 should give an MSE of about 0.1767. \n",
    "\n",
    "### Note\n",
    "\n",
    "Because a TCN's receptive field depends on depth of the network and the filter size, we need\n",
    "to make sure these the model we use can cover the sequence length T. \n",
    "\n",
    "## Data generator for adding problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def data_generator(N, seq_length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_length: Length of the adding problem data\n",
    "        N: # of data in the set\n",
    "    \"\"\"\n",
    "    X_num = torch.rand([N, 1, seq_length])\n",
    "    X_mask = torch.zeros([N, 1, seq_length])\n",
    "    Y = torch.zeros([N, 1])\n",
    "    for i in range(N):\n",
    "        positions = np.random.choice(seq_length, size=2, replace=False)\n",
    "        X_mask[i, 0, positions[0]] = 1\n",
    "        X_mask[i, 0, positions[1]] = 1\n",
    "        Y[i,0] = X_num[i, 0, positions[0]] + X_num[i, 0, positions[1]]\n",
    "    X = torch.cat((X_num, X_mask), dim=1)\n",
    "    return Variable(X), Variable(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN for adding problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.tcn(x)\n",
    "        return self.linear(y1[:, :, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate TCN on Adding problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "# Manually define configuration variables\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # For Apple Silicon Macs\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "use_cuda = device.type == \"cuda\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "batch_size = 32\n",
    "dropout = 0.0\n",
    "clip = -1\n",
    "epochs = 10\n",
    "ksize = 7\n",
    "levels = 8\n",
    "seq_len = 400\n",
    "log_interval = 100\n",
    "lr = 4e-3\n",
    "optim_choice = 'Adam'\n",
    "nhid = 30\n",
    "seed = 1111\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    if not use_cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably enable CUDA\")\n",
    "\n",
    "input_channels = 2\n",
    "n_classes = 1\n",
    "\n",
    "print(\"Producing data...\")\n",
    "X_train, Y_train = data_generator(50000, seq_len)\n",
    "X_test, Y_test = data_generator(1000, seq_len)\n",
    "\n",
    "# Define model architecture\n",
    "channel_sizes = [nhid] * levels\n",
    "kernel_size = ksize\n",
    "model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout)\n",
    "\n",
    "# Move data and model to GPU if CUDA is enabled\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    X_train = X_train.cuda()\n",
    "    Y_train = Y_train.cuda()\n",
    "    X_test = X_test.cuda()\n",
    "    Y_test = Y_test.cuda()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = getattr(optim, optim_choice)(model.parameters(), lr=lr)\n",
    "\n",
    "def train(epoch):\n",
    "    global lr\n",
    "    model.train()\n",
    "    batch_idx = 1\n",
    "    total_loss = 0\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        if i + batch_size > X_train.size(0):\n",
    "            x, y = X_train[i:], Y_train[i:]\n",
    "        else:\n",
    "            x, y = X_train[i:(i+batch_size)], Y_train[i:(i+batch_size)]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = F.mse_loss(output, y)\n",
    "        loss.backward()\n",
    "        if clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        batch_idx += 1\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            processed = min(i+batch_size, X_train.size(0))\n",
    "            print('Train Epoch: {:2d} [{:6d}/{:6d} ({:.0f}%)]\\tLearning rate: {:.4f}\\tLoss: {:.6f}'.format(\n",
    "                epoch, processed, X_train.size(0), 100.*processed/X_train.size(0), lr, cur_loss))\n",
    "            total_loss = 0\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test)\n",
    "        test_loss = F.mse_loss(output, Y_test)\n",
    "        print('\\nTest set: Average loss: {:.6f}\\n'.format(test_loss.item()))\n",
    "        return test_loss.item()\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    train(ep)\n",
    "    tloss = evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Char problem\n",
    "\n",
    "## Character-level Language Modeling\n",
    "\n",
    "### Overview\n",
    "\n",
    "In character-level language modeling tasks, each sequence is broken into elements by characters. \n",
    "Therefore, in a character-level language model, at each time step the model is expected to predict\n",
    "the next coming character. We evaluate the temporal convolutional network as a character-level\n",
    "language model on the PennTreebank dataset and the text8 dataset.\n",
    "\n",
    "### Data\n",
    "\n",
    "- **PennTreebank**: When used as a character-level lan-\n",
    "guage corpus, PTB contains 5,059K characters for training,\n",
    "396K for validation, and 446K for testing, with an alphabet\n",
    "size of 50. PennTreebank is a well-studied (but relatively\n",
    "small) language dataset.\n",
    "\n",
    "- **text8**: text8 is about 20 times larger than PTB, with \n",
    "about 100M characters from Wikipedia (90M for training, 5M \n",
    "for validation, and 5M for testing). The corpus contains 27 \n",
    "unique alphabets.\n",
    "\n",
    "See `data_generator` in `utils.py`. We download the language corpus using [observations](#) package \n",
    "in python.\n",
    "\n",
    "\n",
    "### Note\n",
    "\n",
    "- Just like in a recurrent network implementation where it is common to repackage \n",
    "hidden units when a new sequence begins, we pass into TCN a sequence `T` consisting \n",
    "of two parts: 1) effective history `L1`, and 2) valid sequence `L2`:\n",
    "\n",
    "```\n",
    "Sequence [---------T---------] = [--L1-- -----L2-----]\n",
    "```\n",
    "\n",
    "In the forward pass, the whole sequence is passed into TCN, but only the `L2` portion is used for \n",
    "training. This ensures that the training data are also provided with sufficient history. The size\n",
    "of `T` and `L2` can be adjusted via flags `seq_len` and `validseqlen`.\n",
    "\n",
    "- The choice of dataset to use can be specified via the `--dataset` flag. For instance, running\n",
    "\n",
    "```\n",
    "python char_cnn_test.py --dataset ptb\n",
    "```\n",
    "\n",
    "would (download if no data found, and) train on the PennTreebank (PTB) dataset.\n",
    "\n",
    "- Empirically, we found that Adam works better than SGD on the text8 dataset.\n",
    "\n",
    "## Data loader / generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from collections import Counter\n",
    "import observations\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def data_generator(args):\n",
    "    file, testfile, valfile = getattr(observations, args[\"dataset\"])('data/')\n",
    "    file_len = len(file)\n",
    "    valfile_len = len(valfile)\n",
    "    testfile_len = len(testfile)\n",
    "    corpus = Corpus(file + \" \" + valfile + \" \" + testfile)\n",
    "\n",
    "    #############################################################\n",
    "    # Use the following if you want to pickle the loaded data\n",
    "    #\n",
    "    # pickle_name = \"{0}.corpus\".format(args.dataset)\n",
    "    # if os.path.exists(pickle_name):\n",
    "    #     corpus = pickle.load(open(pickle_name, 'rb'))\n",
    "    # else:\n",
    "    #     corpus = Corpus(file + \" \" + valfile + \" \" + testfile)\n",
    "    #     pickle.dump(corpus, open(pickle_name, 'wb'))\n",
    "    #############################################################\n",
    "\n",
    "    return file, file_len, valfile, valfile_len, testfile, testfile_len, corpus\n",
    "\n",
    "\n",
    "def read_file(filename):\n",
    "    file = unidecode.unidecode(open(filename).read())\n",
    "    return file, len(file)\n",
    "\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.char2idx = {}\n",
    "        self.idx2char = []\n",
    "        self.counter = Counter()\n",
    "\n",
    "    def add_word(self, char):\n",
    "        self.counter[char] += 1\n",
    "\n",
    "    def prep_dict(self):\n",
    "        for char in self.counter:\n",
    "            if char not in self.char2idx:\n",
    "                self.idx2char.append(char)\n",
    "                self.char2idx[char] = len(self.idx2char) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2char)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, string):\n",
    "        self.dict = Dictionary()\n",
    "        for c in string:\n",
    "            self.dict.add_word(c)\n",
    "        self.dict.prep_dict()\n",
    "\n",
    "\n",
    "def char_tensor(corpus, string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for i in range(len(string)):\n",
    "        tensor[i] = corpus.dict.char2idx[string[i]]\n",
    "    return Variable(tensor).cuda() if cuda else Variable(tensor)\n",
    "\n",
    "\n",
    "def batchify(data, batch_size, args):\n",
    "    \"\"\"The output should have size [L x batch_size], where L could be a long sequence length\"\"\"\n",
    "    # Work out how cleanly we can divide the dataset into batch_size parts (i.e. continuous seqs).\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # Evenly divide the data across the batch_size batches.\n",
    "    data = data.view(batch_size, -1)\n",
    "    if args[\"cuda\"]:\n",
    "        data = data.cuda()\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(source, start_index, args):\n",
    "    seq_len = min(args[\"seq_len\"], source.size(1) - 1 - start_index)\n",
    "    end_index = start_index + seq_len\n",
    "    inp = source[:, start_index:end_index].contiguous()\n",
    "    target = source[:, start_index+1:end_index+1].contiguous()  # The successors of the inp.\n",
    "    return inp, target\n",
    "\n",
    "\n",
    "def save(model):\n",
    "    save_filename = 'model.pt'\n",
    "    torch.save(model, save_filename)\n",
    "    print('Saved as %s' % save_filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN for Char problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import sys\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size=2, dropout=0.2, emb_dropout=0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        self.encoder = nn.Embedding(output_size, input_size)\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.decoder = nn.Linear(input_size, output_size)\n",
    "        self.decoder.weight = self.encoder.weight\n",
    "        self.drop = nn.Dropout(emb_dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input has dimension (N, L_in), and emb has dimension (N, L_in, C_in)\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        y = self.tcn(emb.transpose(1, 2))\n",
    "        o = self.decoder(y.transpose(1, 2))\n",
    "        return o.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate TCN on Char problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")   # Suppress the RunTimeWarning on unicode\n",
    "\n",
    "# Replace parser with a dictionary of variables\n",
    "args = {\n",
    "    'batch_size': 32,\n",
    "    'cuda': False,\n",
    "    'dropout': 0.1,\n",
    "    'emb_dropout': 0.1,\n",
    "    'clip': 0.15,\n",
    "    'epochs': 10,\n",
    "    'ksize': 3,\n",
    "    'levels': 3,\n",
    "    'log_interval': 100,\n",
    "    'lr': 4.0,\n",
    "    'emsize': 100,\n",
    "    'optim': 'SGD',\n",
    "    'nhid': 450,\n",
    "    'validseqlen': 320,\n",
    "    'seq_len': 400,\n",
    "    'seed': 1111,\n",
    "    'dataset': 'ptb'\n",
    "}\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "# Check for available accelerators\n",
    "device = torch.device('cpu')  # Default to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    args['cuda'] = True\n",
    "    print(\"CUDA is available. Running with CUDA.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"MPS is available. Running with MPS.\")\n",
    "else:\n",
    "    print(\"No GPU accelerator is available. Running with CPU.\")\n",
    "\n",
    "print(args)\n",
    "file, file_len, valfile, valfile_len, testfile, testfile_len, corpus = data_generator(args)\n",
    "\n",
    "n_characters = len(corpus.dict)\n",
    "train_data = batchify(char_tensor(corpus, file), args['batch_size'], args)\n",
    "val_data = batchify(char_tensor(corpus, valfile), 1, args)\n",
    "test_data = batchify(char_tensor(corpus, testfile), 1, args)\n",
    "print(\"Corpus size: \", n_characters)\n",
    "\n",
    "num_chans = [args['nhid']] * (args['levels'] - 1) + [args['emsize']]\n",
    "k_size = args['ksize']\n",
    "dropout = args['dropout']\n",
    "emb_dropout = args['emb_dropout']\n",
    "model = TCN(args['emsize'], n_characters, num_chans, kernel_size=k_size, dropout=dropout, emb_dropout=emb_dropout)\n",
    "\n",
    "model.to(device)  # Move model to the appropriate device\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = args['lr']\n",
    "optimizer = getattr(optim, args['optim'])(model.parameters(), lr=lr)\n",
    "\n",
    "def evaluate(source):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    source_len = source.size(1)\n",
    "    with torch.no_grad():\n",
    "        for batch, i in enumerate(range(0, source_len - 1, args['validseqlen'])):\n",
    "            if i + args['seq_len'] - args['validseqlen'] >= source_len:\n",
    "                continue\n",
    "            inp, target = get_batch(source, i, args)\n",
    "            inp, target = inp.to(device), target.to(device)  # Move to device\n",
    "            output = model(inp)\n",
    "            eff_history = args['seq_len'] - args['validseqlen']\n",
    "            final_output = output[:, eff_history:].contiguous().view(-1, n_characters)\n",
    "            final_target = target[:, eff_history:].contiguous().view(-1)\n",
    "            loss = criterion(final_output, final_target)\n",
    "\n",
    "            total_loss += loss.data * final_output.size(0)\n",
    "            count += final_output.size(0)\n",
    "\n",
    "        val_loss = total_loss.item() / count * 1.0\n",
    "        return val_loss\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    losses = []\n",
    "    source = train_data\n",
    "    source_len = source.size(1)\n",
    "    for batch_idx, i in enumerate(range(0, source_len - 1, args['validseqlen'])):\n",
    "        if i + args['seq_len'] - args['validseqlen'] >= source_len:\n",
    "            continue\n",
    "        inp, target = get_batch(source, i, args)\n",
    "        inp, target = inp.to(device), target.to(device)  # Move to device\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inp)\n",
    "        eff_history = args['seq_len'] - args['validseqlen']\n",
    "        final_output = output[:, eff_history:].contiguous().view(-1, n_characters)\n",
    "        final_target = target[:, eff_history:].contiguous().view(-1)\n",
    "        loss = criterion(final_output, final_target)\n",
    "        loss.backward()\n",
    "\n",
    "        if args['clip'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip'])\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % args['log_interval'] == 0 and batch_idx > 0:\n",
    "            cur_loss = total_loss / args['log_interval']\n",
    "            losses.append(cur_loss)\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.5f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.3f} | bpc {:5.3f}'.format(\n",
    "                epoch, batch_idx, int((source_len-0.5) / args['validseqlen']), lr,\n",
    "                              elapsed * 1000 / args['log_interval'], cur_loss, cur_loss / math.log(2)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "    return sum(losses) * 1.0 / len(losses)\n",
    "\n",
    "def main():\n",
    "    global lr\n",
    "    try:\n",
    "        print(\"Training for %d epochs...\" % args['epochs'])\n",
    "        all_losses = []\n",
    "        best_vloss = 1e7\n",
    "        for epoch in range(1, args['epochs'] + 1):\n",
    "            loss = train(epoch)\n",
    "\n",
    "            vloss = evaluate(val_data)\n",
    "            print('-' * 89)\n",
    "            print('| End of epoch {:3d} | valid loss {:5.3f} | valid bpc {:8.3f}'.format(\n",
    "                epoch, vloss, vloss / math.log(2)))\n",
    "\n",
    "            test_loss = evaluate(test_data)\n",
    "            print('=' * 89)\n",
    "            print('| End of epoch {:3d} | test loss {:5.3f} | test bpc {:8.3f}'.format(\n",
    "                epoch, test_loss, test_loss / math.log(2)))\n",
    "            print('=' * 89)\n",
    "\n",
    "            if epoch > 5 and vloss > max(all_losses[-3:]):\n",
    "                lr = lr / 10.\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr\n",
    "            all_losses.append(vloss)\n",
    "\n",
    "            if vloss < best_vloss:\n",
    "                print(\"Saving...\")\n",
    "                save(model)\n",
    "                best_vloss = vloss\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('-' * 89)\n",
    "        print(\"Saving before quit...\")\n",
    "        save(model)\n",
    "\n",
    "    # Run on test data.\n",
    "    test_loss = evaluate(test_data)\n",
    "    print('=' * 89)\n",
    "    print('| End of training | test loss {:5.3f} | test bpc {:8.3f}'.format(\n",
    "        test_loss, test_loss / math.log(2)))\n",
    "    print('=' * 89)\n",
    "\n",
    "# train_by_random_chunk()\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: MNIST Pixel problem\n",
    "\n",
    "## Sequential MNIST & Permuted Sequential MNIST\n",
    "\n",
    "### Overview\n",
    "\n",
    "MNIST is a handwritten digit classification dataset (Lecun et al., 1998) that is frequently used to \n",
    "test deep learning models. In particular, sequential MNIST is frequently used to test a recurrent \n",
    "network’s ability to retain information from the distant past (see paper for references). In \n",
    "this task, each MNIST image (28 x 28) is presented to the model as a 784 × 1 sequence \n",
    "for digit classification. In the more challenging permuted MNIST (P-MNIST) setting, the order of \n",
    "the sequence is permuted at a (fixed) random order.\n",
    "\n",
    "### Note\n",
    "\n",
    "- Because a TCN's receptive field depends on depth of the network and the filter size, we need\n",
    "to make sure these the model we use can cover the sequence length 784. \n",
    "\n",
    "- While this is a sequence model task, we only use the very last output (i.e. at time T=784) for \n",
    "the eventual classification.\n",
    "\n",
    "## data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def data_generator(root, batch_size):\n",
    "    train_set = datasets.MNIST(root=root, train=True, download=True,\n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "                               ]))\n",
    "    test_set = datasets.MNIST(root=root, train=False, download=True,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ]))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN for MNIST Pixel problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Inputs have to have dimension (N, C_in, L_in)\"\"\"\n",
    "        y1 = self.tcn(inputs)  # input should have dimension (N, C, L)\n",
    "        o = self.linear(y1[:, :, -1])\n",
    "        return F.log_softmax(o, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate TCN on MNIST Pixel problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Replace parser with a dictionary of variables\n",
    "args = {\n",
    "    'batch_size': 64,\n",
    "    'dropout': 0.05,\n",
    "    'clip': -1,\n",
    "    'epochs': 10,\n",
    "    'ksize': 7,\n",
    "    'levels': 8,\n",
    "    'log_interval': 100,\n",
    "    'lr': 2e-3,\n",
    "    'optim': 'Adam',\n",
    "    'nhid': 25,\n",
    "    'seed': 1111,\n",
    "    'permute': False\n",
    "}\n",
    "\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "# Check for available accelerators\n",
    "device = torch.device('cpu')  # Default to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"CUDA is available. Running with CUDA.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"MPS is available. Running with MPS.\")\n",
    "else:\n",
    "    print(\"No GPU accelerator is available. Running with CPU.\")\n",
    "\n",
    "root = './data/mnist'\n",
    "batch_size = args['batch_size']\n",
    "n_classes = 10\n",
    "input_channels = 1\n",
    "seq_length = int(784 / input_channels)\n",
    "epochs = args['epochs']\n",
    "steps = 0\n",
    "\n",
    "print(args)\n",
    "train_loader, test_loader = data_generator(root, batch_size)\n",
    "\n",
    "permute = torch.Tensor(np.random.permutation(784).astype(np.float64)).long().to(device)\n",
    "channel_sizes = [args['nhid']] * args['levels']\n",
    "kernel_size = args['ksize']\n",
    "model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=args['dropout'])\n",
    "model.to(device)  # Move model to the appropriate device\n",
    "\n",
    "lr = args['lr']\n",
    "optimizer = getattr(optim, args['optim'])(model.parameters(), lr=lr)\n",
    "\n",
    "def train(ep):\n",
    "    global steps\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)  # Move to device\n",
    "        data = data.view(-1, input_channels, seq_length)\n",
    "        if args['permute']:\n",
    "            data = data[:, :, permute]\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        if args['clip'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip'])\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "        steps += seq_length\n",
    "        if batch_idx > 0 and batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tSteps: {}'.format(\n",
    "                ep, batch_idx * batch_size, len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), train_loss.item() / args['log_interval'], steps))\n",
    "            train_loss = 0\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)  # Move to device\n",
    "            data = data.view(-1, input_channels, seq_length)\n",
    "            if args['permute']:\n",
    "                data = data[:, :, permute]\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "        return test_loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train(epoch)\n",
    "        test()\n",
    "        if epoch % 10 == 0:\n",
    "            lr /= 10\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "                \n",
    "import matplotlib.pyplot as plt  # Import matplotlib for plotting\n",
    "def plot_samples(data_loader, num_samples=5):\n",
    "    \"\"\"Plot a few samples from the dataset.\"\"\"\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    # Plot the first `num_samples` images\n",
    "    for i in range(num_samples):\n",
    "        image = images[i].view(28, 28)  # Reshape to 28x28 for visualization\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f\"Label: {labels[i].item()}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "plot_samples(train_loader, num_samples=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Poly Music\n",
    "\n",
    "## Polyphonic Music Dataset\n",
    "\n",
    "### Overview\n",
    "\n",
    "We evaluate temporal convolutional network (TCN) on two popular polyphonic music dataset, described below.\n",
    "\n",
    "- **JSB Chorales** dataset (Allan & Williams, 2005) is a polyphonic music dataset con-\n",
    "sisting of the entire corpus of 382 four-part harmonized chorales by J. S. Bach. In a polyphonic\n",
    "music dataset, each input is a sequence of elements having 88 dimensions, representing the 88 keys\n",
    "on a piano. Therefore, each element `x_t` is a chord written in as binary vector, in which a “1” indicates\n",
    "a key pressed.\n",
    "\n",
    "- **Nottingham** dataset is a collection of 1200 British and American folk tunes. Not-\n",
    "tingham is a much larger dataset than JSB Chorales. Along with JSB Chorales, Nottingham has\n",
    "been used in a number of works that investigated recurrent models’ applicability in polyphonic mu-\n",
    "sic, and the performance for both tasks are measured in terms\n",
    "of negative log-likelihood (NLL) loss.\n",
    "\n",
    "The goal here is to predict the next note given some history of the notes played.\n",
    "\n",
    "### Note\n",
    "\n",
    "- Each sequence can have a different length. In the current implementation, we simply train each\n",
    "sequence separately (i.e. batch size is 1), but one can zero-pad all sequences to the same length\n",
    "and train by batch.\n",
    "\n",
    "- One can use different datasets by specifying through the `--data` flag on the command line. The\n",
    "default is `Nott`, for Nottingham.\n",
    "\n",
    "- While each data is binary, the fact that there are 88 dimensions (for 88 keys) means there are\n",
    "essentially `2^88` \"classes\". Therefore, instead of directly predicting each key directly, we\n",
    "follow the standard practice so that a sigmoid is added at the end of the network. This ensures\n",
    "that every entry is converted to a value between 0 and 1 to compute the NLL loss.\n",
    "\n",
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def data_generator(dataset):\n",
    "    if dataset == \"JSB\":\n",
    "        print('loading JSB data...')\n",
    "        data = loadmat('./TCN/poly_music/mdata/JSB_Chorales.mat')\n",
    "    elif dataset == \"Muse\":\n",
    "        print('loading Muse data...')\n",
    "        data = loadmat('./TCN/poly_music/mdata/MuseData.mat')\n",
    "    elif dataset == \"Nott\":\n",
    "        print('loading Nott data...')\n",
    "        data = loadmat('./TCN/poly_music/mdata/Nottingham.mat')\n",
    "    elif dataset == \"Piano\":\n",
    "        print('loading Piano data...')\n",
    "        data = loadmat('./TCN/poly_music/mdata/Piano_midi.mat')\n",
    "\n",
    "    X_train = data['traindata'][0]\n",
    "    X_valid = data['validdata'][0]\n",
    "    X_test = data['testdata'][0]\n",
    "\n",
    "    for data in [X_train, X_valid, X_test]:\n",
    "        for i in range(len(data)):\n",
    "            data[i] = torch.Tensor(data[i].astype(np.float64))\n",
    "\n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN for Poly Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
    "        super(TCN, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x needs to have dimension (N, C, L) in order to be passed into CNN\n",
    "        output = self.tcn(x.transpose(1, 2)).transpose(1, 2)\n",
    "        output = self.linear(output).double()\n",
    "        return self.sig(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate TCN on Poly Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Replace argparse with dictionary for configuration\n",
    "args = {\n",
    "    'dropout': 0.25,\n",
    "    'clip': 0.2,\n",
    "    'epochs': 10,\n",
    "    'ksize': 5,\n",
    "    'levels': 4,\n",
    "    'log_interval': 100,\n",
    "    'lr': 1e-3,\n",
    "    'optim': 'Adam',\n",
    "    'nhid': 150,\n",
    "    'data': 'Nott',\n",
    "    'seed': 1111\n",
    "}\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "# Check for available accelerators\n",
    "device = torch.device('cpu')  # Default to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"CUDA is available. Running with CUDA.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"MPS is available. Running with MPS.\")\n",
    "else:\n",
    "    print(\"No GPU accelerator is available. Running with CPU.\")\n",
    "\n",
    "print(args)\n",
    "input_size = 88\n",
    "X_train, X_valid, X_test = data_generator(args['data'])\n",
    "\n",
    "n_channels = [args['nhid']] * args['levels']\n",
    "kernel_size = args['ksize']\n",
    "dropout = args['dropout']\n",
    "\n",
    "model = TCN(input_size, input_size, n_channels, kernel_size, dropout=args['dropout'])\n",
    "model.to(device)  # Move model to the appropriate device\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = args['lr']\n",
    "optimizer = getattr(optim, args['optim'])(model.parameters(), lr=lr)\n",
    "\n",
    "def evaluate(X_data, name='Eval'):\n",
    "    model.eval()\n",
    "    eval_idx_list = np.arange(len(X_data), dtype=\"int32\")\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for idx in eval_idx_list:\n",
    "            data_line = X_data[idx]\n",
    "            x, y = Variable(data_line[:-1]).to(device), Variable(data_line[1:]).to(device)\n",
    "            output = model(x.unsqueeze(0)).squeeze(0)\n",
    "            loss = -torch.trace(torch.matmul(y, torch.log(output).float().t()) +\n",
    "                                torch.matmul((1-y), torch.log(1-output).float().t()))\n",
    "            total_loss += loss.item()\n",
    "            count += output.size(0)\n",
    "        eval_loss = total_loss / count\n",
    "        print(name + \" loss: {:.5f}\".format(eval_loss))\n",
    "        return eval_loss\n",
    "\n",
    "def train(ep):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    train_idx_list = np.arange(len(X_train), dtype=\"int32\")\n",
    "    np.random.shuffle(train_idx_list)\n",
    "    for idx in train_idx_list:\n",
    "        data_line = X_train[idx]\n",
    "        x, y = Variable(data_line[:-1]).to(device), Variable(data_line[1:]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x.unsqueeze(0)).squeeze(0)\n",
    "        loss = -torch.trace(torch.matmul(y, torch.log(output).float().t()) +\n",
    "                            torch.matmul((1 - y), torch.log(1 - output).float().t()))\n",
    "        total_loss += loss.item()\n",
    "        count += output.size(0)\n",
    "\n",
    "        if args['clip'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args['clip'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx > 0 and idx % args['log_interval'] == 0:\n",
    "            cur_loss = total_loss / count\n",
    "            print(\"Epoch {:2d} | lr {:.5f} | loss {:.5f}\".format(ep, lr, cur_loss))\n",
    "            total_loss = 0.0\n",
    "            count = 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best_vloss = 1e8\n",
    "    vloss_list = []\n",
    "    model_name = \"poly_music_{0}.pt\".format(args['data'])\n",
    "    for ep in range(1, args['epochs']+1):\n",
    "        train(ep)\n",
    "        vloss = evaluate(X_valid, name='Validation')\n",
    "        tloss = evaluate(X_test, name='Test')\n",
    "        if vloss < best_vloss:\n",
    "            with open(model_name, \"wb\") as f:\n",
    "                torch.save(model, f)\n",
    "                print(\"Saved model!\\n\")\n",
    "            best_vloss = vloss\n",
    "        if ep > 10 and vloss > max(vloss_list[-3:]):\n",
    "            lr /= 10\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "        vloss_list.append(vloss)\n",
    "\n",
    "    print('-' * 89)\n",
    "    model = torch.load(open(model_name, \"rb\"))\n",
    "    tloss = evaluate(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
